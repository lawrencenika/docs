---
title: "NikaETL"
description: "One click deploy python script as serverless function, and use AI agent to create a flow diagram using these functions for data processing engineering"
---

## Overview

NikaETL is a powerful data processing engineering platform that enables you to deploy Python scripts as serverless functions with one click, and leverage AI agents to create intelligent flow diagrams for complex data processing pipelines. Built for data engineers and analysts who need scalable, automated ETL workflows.

![NikaETL Interface](/products/nika-etl.jpg)

## Key Features

### One-Click Serverless Deployment
- **Instant Deployment**: Deploy Python scripts as serverless functions with a single click
- **Auto-Scaling**: Automatic scaling based on workload demands
- **Pay-Per-Use**: Only pay for actual function execution time
- **Zero Infrastructure**: No server management or configuration required

### AI-Powered Flow Design
- **Intelligent Flow Creation**: AI agent automatically creates optimal flow diagrams
- **Smart Function Chaining**: AI suggests the best way to connect functions
- **Performance Optimization**: AI recommends optimizations for data processing
- **Error Handling**: AI-generated error handling and recovery mechanisms

### Data Processing Engineering
- **ETL Workflows**: Extract, Transform, Load data processing pipelines
- **Data Validation**: Built-in data quality checks and validation
- **Transformation Tools**: Rich library of data transformation functions
- **Monitoring**: Real-time monitoring and alerting for data pipelines

### Integration Capabilities
- **Multiple Data Sources**: Connect to databases, APIs, cloud storage, and more
- **Format Support**: Handle CSV, JSON, Parquet, Avro, and other formats
- **Geospatial Data**: Specialized support for spatial data processing
- **Real-time Processing**: Stream processing capabilities for live data

## Getting Started

### Deploying Your First Function

```python
# example_etl_function.py
import pandas as pd
import geopandas as gpd
from nika_etl import ETLFunction

@ETLFunction
def process_geospatial_data(input_data):
    """
    Process geospatial data with automatic serverless deployment
    """
    # Load data
    df = pd.read_csv(input_data['csv_url'])
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))
    
    # Transform data
    gdf = gdf.to_crs(epsg=4326)
    gdf['processed'] = gdf['value'] * 2
    
    # Return processed data
    return {
        'processed_data': gdf.to_json(),
        'summary': {
            'total_records': len(gdf),
            'bounds': gdf.total_bounds.tolist()
        }
    }

# Deploy with one click
# nika etl deploy example_etl_function.py
```

### Creating AI-Powered Flows

```python
from nika_etl import ETLFlow, AIAgent

# Initialize AI agent for flow design
ai_agent = AIAgent()

# Describe your data processing requirements
flow_description = """
Create a data processing pipeline that:
1. Extracts data from PostgreSQL database
2. Cleans and validates the data
3. Performs geospatial analysis
4. Loads results into a data warehouse
5. Sends notifications on completion
"""

# AI creates the optimal flow diagram
flow = await ai_agent.create_flow(flow_description)

# Deploy the flow
etl_flow = ETLFlow(flow)
await etl_flow.deploy()
```

## Function Development

### Python Function Templates

```python
# Data extraction function
@ETLFunction
def extract_from_database(connection_string, query):
    """Extract data from database"""
    import psycopg2
    import pandas as pd
    
    conn = psycopg2.connect(connection_string)
    df = pd.read_sql(query, conn)
    conn.close()
    
    return {'data': df.to_dict('records')}

# Data transformation function
@ETLFunction
def transform_geospatial_data(data):
    """Transform geospatial data"""
    import geopandas as gpd
    import pandas as pd
    
    df = pd.DataFrame(data)
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lng, df.lat))
    
    # Perform spatial operations
    gdf = gdf.to_crs(epsg=3857)
    gdf['area'] = gdf.geometry.area
    
    return {'transformed_data': gdf.to_dict('records')}

# Data loading function
@ETLFunction
def load_to_warehouse(data, warehouse_config):
    """Load data to warehouse"""
    import pandas as pd
    from sqlalchemy import create_engine
    
    df = pd.DataFrame(data)
    engine = create_engine(warehouse_config['connection_string'])
    df.to_sql(warehouse_config['table'], engine, if_exists='append')
    
    return {'loaded_records': len(df)}
```

### Function Configuration

```python
# Function configuration
@ETLFunction(
    name="geospatial_processor",
    memory="2GB",
    timeout=300,
    environment={
        "GDAL_VERSION": "3.9.1",
        "PROJ_VERSION": "9.3.1"
    },
    dependencies=[
        "geopandas>=0.13.0",
        "shapely>=2.0.0",
        "pyproj>=3.6.0"
    ]
)
def process_large_geospatial_dataset(input_data):
    """Process large geospatial datasets with optimized memory usage"""
    # Function implementation
    pass
```

## AI-Powered Flow Design

### Natural Language Flow Creation

```python
# Describe your workflow in natural language
workflow_description = """
Create an ETL pipeline for urban planning data that:
1. Extracts building footprints from OpenStreetMap
2. Calculates building density by neighborhood
3. Identifies areas suitable for development
4. Generates a report with recommendations
5. Sends alerts for high-priority areas
"""

# AI generates the complete flow
flow = await ai_agent.create_flow(
    description=workflow_description,
    optimization="performance",
    monitoring=True
)

print("Generated Flow:")
for step in flow.steps:
    print(f"- {step.name}: {step.function}")
    print(f"  Input: {step.inputs}")
    print(f"  Output: {step.outputs}")
```

### Flow Optimization

```python
# AI optimizes existing flow
optimized_flow = await ai_agent.optimize_flow(
    flow_id="existing-flow-123",
    optimization_target="cost",
    constraints={
        "max_execution_time": 3600,
        "max_memory": "4GB"
    }
)

# AI suggests improvements
suggestions = await ai_agent.analyze_flow(flow_id="existing-flow-123")
print("Optimization Suggestions:")
for suggestion in suggestions:
    print(f"- {suggestion.type}: {suggestion.description}")
    print(f"  Expected improvement: {suggestion.improvement}")
```

## Data Processing Pipelines

### ETL Workflow Examples

```python
# Complete ETL pipeline
@ETLFlow
class UrbanPlanningPipeline:
    
    @ETLFunction
    def extract_osm_data(self, area_bounds):
        """Extract OpenStreetMap data for specified area"""
        import osmnx as ox
        
        # Extract building data
        buildings = ox.geometries_from_bbox(
            area_bounds['north'],
            area_bounds['south'],
            area_bounds['east'],
            area_bounds['west'],
            tags={'building': True}
        )
        
        return {'buildings': buildings.to_dict('records')}
    
    @ETLFunction
    def calculate_density(self, buildings_data, neighborhood_boundaries):
        """Calculate building density by neighborhood"""
        import geopandas as gpd
        import pandas as pd
        
        buildings_gdf = gpd.GeoDataFrame(buildings_data)
        neighborhoods_gdf = gpd.GeoDataFrame(neighborhood_boundaries)
        
        # Spatial join to assign buildings to neighborhoods
        joined = gpd.sjoin(buildings_gdf, neighborhoods_gdf, how='left')
        
        # Calculate density metrics
        density_stats = joined.groupby('neighborhood_id').agg({
            'building_area': ['sum', 'mean', 'count'],
            'building_height': ['mean', 'max']
        })
        
        return {'density_stats': density_stats.to_dict()}
    
    @ETLFunction
    def identify_development_areas(self, density_stats, criteria):
        """Identify areas suitable for development"""
        import pandas as pd
        
        df = pd.DataFrame(density_stats)
        
        # Apply development criteria
        suitable_areas = df[
            (df['building_density'] < criteria['max_density']) &
            (df['avg_building_age'] > criteria['min_age']) &
            (df['infrastructure_score'] > criteria['min_infrastructure'])
        ]
        
        return {'suitable_areas': suitable_areas.to_dict('records')}
    
    @ETLFunction
    def generate_report(self, analysis_results):
        """Generate comprehensive report"""
        import json
        from datetime import datetime
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_neighborhoods': len(analysis_results['density_stats']),
                'suitable_areas': len(analysis_results['suitable_areas']),
                'total_buildings': sum(stats['building_count'] for stats in analysis_results['density_stats'].values())
            },
            'recommendations': analysis_results['suitable_areas'],
            'visualizations': {
                'density_map': 'url_to_density_map',
                'development_priority': 'url_to_priority_map'
            }
        }
        
        return {'report': report}
    
    @ETLFunction
    def send_notifications(self, report):
        """Send notifications for high-priority areas"""
        import smtplib
        from email.mime.text import MIMEText
        
        high_priority = [area for area in report['recommendations'] 
                        if area['priority_score'] > 0.8]
        
        if high_priority:
            # Send email notification
            msg = MIMEText(f"High priority development areas identified: {len(high_priority)}")
            msg['Subject'] = 'Urban Planning Alert'
            msg['From'] = 'nika-etl@example.com'
            msg['To'] = 'planners@city.gov'
            
            # Send email (configure SMTP settings)
            # smtp.send_message(msg)
        
        return {'notifications_sent': len(high_priority)}
```

## Monitoring and Observability

### Real-time Monitoring

```python
# Monitor flow execution
@ETLFunction
def monitor_flow_execution(flow_id):
    """Monitor flow execution in real-time"""
    from nika_etl import FlowMonitor
    
    monitor = FlowMonitor(flow_id)
    
    # Get execution metrics
    metrics = monitor.get_metrics()
    
    # Check for failures
    failures = monitor.get_failures()
    
    # Get performance data
    performance = monitor.get_performance()
    
    return {
        'status': monitor.get_status(),
        'metrics': metrics,
        'failures': failures,
        'performance': performance
    }
```

### Alerting and Notifications

```python
# Set up alerts
@ETLFunction
def setup_alerts(flow_id, alert_config):
    """Set up monitoring alerts"""
    from nika_etl import AlertManager
    
    alert_manager = AlertManager(flow_id)
    
    # Configure alerts
    alert_manager.add_alert(
        name="execution_time",
        condition="execution_time > 3600",
        action="send_email",
        recipients=["admin@example.com"]
    )
    
    alert_manager.add_alert(
        name="data_quality",
        condition="failed_records > 100",
        action="slack_notification",
        channel="#data-alerts"
    )
    
    return {'alerts_configured': True}
```

## Integration

### Data Source Connections

```python
# Database connections
@ETLFunction
def connect_postgresql(connection_string):
    """Connect to PostgreSQL database"""
    import psycopg2
    
    conn = psycopg2.connect(connection_string)
    return {'connection': conn}

# API connections
@ETLFunction
def connect_api(api_config):
    """Connect to external API"""
    import requests
    
    session = requests.Session()
    session.headers.update(api_config['headers'])
    
    return {'session': session}

# Cloud storage connections
@ETLFunction
def connect_s3(bucket_name, credentials):
    """Connect to S3 bucket"""
    import boto3
    
    s3_client = boto3.client(
        's3',
        aws_access_key_id=credentials['access_key'],
        aws_secret_access_key=credentials['secret_key']
    )
    
    return {'s3_client': s3_client, 'bucket': bucket_name}
```

## API Reference

### Function Management
- `deploy_function()`: Deploy Python function as serverless
- `update_function()`: Update deployed function
- `delete_function()`: Delete deployed function
- `list_functions()`: List all deployed functions

### Flow Management
- `create_flow()`: Create new ETL flow
- `deploy_flow()`: Deploy flow to production
- `update_flow()`: Update existing flow
- `delete_flow()`: Delete flow
- `list_flows()`: List all flows

### AI Agent
- `create_flow()`: AI creates flow from description
- `optimize_flow()`: AI optimizes existing flow
- `analyze_flow()`: AI analyzes flow performance
- `suggest_improvements()`: AI suggests improvements

### Monitoring
- `get_metrics()`: Get flow execution metrics
- `get_failures()`: Get flow failures
- `get_performance()`: Get performance data
- `setup_alerts()`: Configure monitoring alerts

## Best Practices

### Function Design
1. **Single Responsibility**: Each function should do one thing well
2. **Error Handling**: Implement comprehensive error handling
3. **Logging**: Add detailed logging for debugging
4. **Testing**: Test functions thoroughly before deployment

### Flow Design
1. **Modularity**: Break complex workflows into smaller functions
2. **Error Recovery**: Implement retry and recovery mechanisms
3. **Monitoring**: Add comprehensive monitoring and alerting
4. **Documentation**: Document flow logic and dependencies

### Performance
1. **Optimization**: Use AI suggestions for performance optimization
2. **Caching**: Implement caching for frequently accessed data
3. **Parallelization**: Use parallel processing where possible
4. **Resource Management**: Monitor and optimize resource usage

## Support

Need help with NikaETL? Check out our [support page](/support) or join our [community forum](/forum). 